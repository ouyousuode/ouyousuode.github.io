---
layout: post
title: 略谈Hashing
---
{{page.title}}
===============================

Data structures are the building blocks of computer algorithms.A design of an algorithm is like a design of a building.One has to put all the rooms together in a way that is the most effective for the intended use of the building.To do that,it is not enough to know about functionality,efficiency,form and beauty.One needs a thorough knowledge of construction techniques.Putting a room in midair may achieve the desired effect,but it is not possible.Other ideas may be possible,but too expensive.In the same way,a design of an algorithm must be based on a thorough understanding of data structure techniques and costs.

But when read **3.7 Hashing and String** in「**The Algorithm Design Manual**」by **Steven S.Skiena**.There is one interesting sentence "Hashing has a variety of clever applications beyond just speeding up searching.I once heard **Udi Manber**—then Chief Scientist at Yahoo—talk about the algorithms employed at his company.The three most important algorithms at Yahoo,he said,were **hashing,hashing,and hashing**."And where does this sentence come from?

<img src="/images/posts/2019-08-15/hashing_by_Udi_Manber.png">

Hashing is one of the most(if not the most)useful data structures for computer algorithms.Hashing is used mainly for insertions and searches,and some variations of it can also be used for deletions.The idea behind hashing is simple.Designing a data structure for storing data with keys numbered from *1* to *n* is easy: The data can be stored in an array of size *n*,such that key *i* is stored at location *i*.And key can thus be accessed immediately.

If there are *n* unique keys in the range *1* to *2n*,for example,then it is still usually best to store them in an array of size *2n*,even though the storage utilization is now only *50* percent.The access is so efficient that it is usually worth the extra space.However,if the keys are integers,say,in the range *1* to *M*,where *M* is the maximal integer that can be represented in the particular computer,we can not afford to allocate space of size *M*.

For example,if there are *250* students identified by their social-security number,we will not allocate an array of size *1* billion to store information about them(there are *1* billion possible social-security numbers).Instead,we can use the last three digits of the numbers,in which case we need only an array size of *1000*.This is not a foolproof method.There may be students with the same last three digits(in fact,with 250 students,the probability of that is quite high).We will show how to handle such duplicates shortly.We can also use the last four digits,or the last three digits and the first letter of the student's name,to minnimize duplicates even further.However,using more digits requires a larger-size table and results in a smaller utilization.

We assume that we are given a set of *n* keys taken from a large set *U* of size *M*,such that *M* is much larger than *n*.We want to store the keys in a table of size *m*,such that *m* is not much larger than *n*.The idea is to use a function,called a **hash function**,to map the keys,which are in the range *1* to *M*,to new keys in the range *1* to *m*,so we can store everything in an array of size *m*.Taking the last three digits of a large integer is such a function.It maps a large set *U* of size 1 billion to a set of size *1000*.Each possible key is thus given a place(index) in a table of size *m*.We will attempt to store the key in that particular place in that table.If the function is easy to compute,then accessing the key is also easy.

However,since the set *U* is large and the table is small,no matter what function we use,many keys will be mapped into the same place in the table.When two keys are mapped into the same location in the table,we call it a **collison**.We are thus faced with two problems:(1)finding a hash function that minimizes the likelihood of collisions,and (2)handling collisions.

Even though the set *U* is much larger than the size of the table,the actual set of keys we hanble is usually not too large.A good hash function should map the keys uniformly in the table.Of course,no hash function can map all possible sets of keys without collisions.If the size of *U* is *M* and the size of the hash table is *m*,then there must be at least *M/m* keys that are mapped into the same place.If the mapping is uniform,each location will have approximately *M/m* keys mapped into it.

Hash functions should transform a set of keys uniformly to a set of *random* locations in the range *1* to *m*.The uniformity and randomness are the essence in hashing.For example,instead of taking the last three digits of the social-security number,we could take the last three digits of the student's year of birth.It's clear that this is an inferior hash function,since it is much more likely that many students were born in the same year than it is that many students have the same last three digits of the social-security number.

## Hash Functions
We assume that the keys are integers,and that the size of the hash table is *m*.A simple and effective hash function is *h(x)= x mod m*,where *m* is a prime number.If the size of the table can not be adjusted easily to be a prime(it is convenient sometimes to have a size that is a power of 2,for example),then the following hash function can be used: *h(x)=(x mod p) mod m*,where *p* is a prime,and *p > m*(*p* should be sufficiently larger than *m* to be effective,but it should also be sufficiently smaller than |*U*|).

As we have already mentioned,no hash function can be good for all inputs.Using primes as described is fairly safe,since most data in practice have no structure related to prime numbers.On the other hand,it is always possible(although unlikely) that,in a certain application,one will want to store results of some experiments made on integers all of which are all of the form *r + kp* for a constant *r*.All these numbers of course will have the same hash values if *p* is used as desribed.

We can take the idea of scrambling data with hashing one step further,and use a random procedure to select a hash function! For example,the prime *p* can be selected at random from a list of primes in the appropriate range.Finding a large list of primes,however,is not easy.Another possibility is the following: At random,select two numbers *a* and *b*,such *a,b < p*,and *a ≠ 0*,and let *h(x)=[ax+b mod p] mod m*.This function is more complicated to compute than the previous one is,but it has the advantage that it is very good on the average for all inputs.

Of course,the same hash function must be used for all accesses to the same table.In many cases,however,there is a need for many independent tables,or tables that are created and destroyed frequently.In those cases,a different hash function can be used every time a different table is created.The random hash functions described above have certain other desirable properties.

## Handling Collisions
The simplest way to handle collisons is to use a method called **separate chaining**.Each entry in the hash table serves as a head of a linked list containing all the keys that are hashed into that entry.To access a key,we hash it and then perform linear search on the appropriate linked list.A new key can be inserted into the beginning of the list(but the list must be searched to ensure that the key is not a duplicate).A search may be inefficient if some lists are long.

The lists will be long if the size of the table is small compared to the actual number of keys or if the hash function is bad.Thus,hashing is not a good dynamic structure.It is important to have a good estimate on the number of keys.The main problem with separate chaining is that it requires dynamic memory allocation and more space for the pointers(even if the number of keys is not too large,and the pointers are not used).On the other hand,if for some reason the estimate of the appropriate table size is wrong,separate hashing will still work,whereas other static methods will fail.

Another simple method is **linear probing**.The size of the table is fixed,and there are no pointers.The hash function determines the place of the key in the table.If that place is already occupied,that is,if a collision occurs,then the first empty place after it is taken instead.A serach for the key follows the same procedure.(The table is considered in a cyclic order;if the last place is reached and it is full,then the first place is considered next.)An unsuccessful search thus ends at the first empty place.When the table is relatively empty,this simple method works well.If the table is relatively full,there will be many **secondary collisions**,which are collisions that are caused by keys with different hash values.We can not avoid collisions with keys that have the same hash function,because such keys are mapped into the same place.We should,however,try to minimize scondary collisions.

Let's look at an example.Suppose that the *i*th place is full and that the (*i+1*)th place is empty.A new key,which is mapped to *i*,will cause a collision,and will be inserted into *i+1*.This case is efficient,since the collision is resolved with minimal effort.However,if a new key is now mapped into *i+1*,there will be a secondary collision and *i+2* will become full(if it is not full already).Any new key mapped to *i*,to *i+1*,or to *i+2* will not encounter secondary colllisions,but will also increase the size of the full segment,causing more secondary collisions later.This effect is called **clustering**.When the table is almost full,the number of secondary collisions with linear probing will be very high,and the search will degrade to linear search.

Deletions can not be implemented efficiently with linear probing.If an insertion "passes" through a key on its way to an empty slot,and if that key is later deleted,then a future search will be unsucessful,since it will stop in the new empty slot.If deletions are required,we must have a collision-resolution scheme using pointers.

The clustering effect can be reduced with **double hashing**.When a collision occurs,a second hash value *h<sub>2</sub>(x)* is computed.Instead of searching in a linear order,namely,*i+1*,*i+2*,and so on,we search the place *i+h<sub>2</sub>(x)*,*i+2h<sub>2</sub>(x)*,and so on(all in a cyclic order).When another key *y* is mapped to,say,*i+h<sub>2</sub>(x)*,the next attempt will be at *i+h<sub>2</sub>(x)+h<sub>2</sub>(y)*,instead of at *i+2h<sub>2</sub>(x)*.If *h<sub>2</sub>(x)* is independent of *h<sub>2</sub>(y)*,then clustering is eliminated.We must be careful,however,to choose the second hash value such that the sequence *i+h<sub>2</sub>(x),i+2h<sub>2</sub>(x),...,i+nh<sub>2</sub>(x)* spans the whole table(which will happen if the numbers *h<sub>2</sub>(x)* and *n* are relatively prime).

The main disadvantage of double hashing is that it requires more computation(namely,the selection of a second hash value) for the search.One way to save extra computation is to select a second hash value that is not completely independent of the first hash value,but that still reduces clustering.One such method is to set *h<sub>2</sub>(x)=1* if *h<sub>1</sub>(x)=0*,and *h<sub>2</sub>(x)=m-h<sub>1</sub>(x)* otherwise (we assume that *m* is prime and that *h<sub>1</sub>(x)=x mod m*).

